{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from youtube_transcript_api import YouTubeTranscriptApi\n",
    "from youtube_transcript_api.formatters import JSONFormatter\n",
    "import whisper\n",
    "import os\n",
    "import yt_dlp\n",
    "from yt_dlp import YoutubeDL\n",
    "import boto3\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_video_id(youtube_url):\n",
    "    \"\"\"\n",
    "    Extracts the video id from a youtube url\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    youtube_url : str\n",
    "        The youtube url\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The video id\n",
    "    \"\"\"\n",
    "    regex = r\"(?:https?:\\/\\/)?(?:www\\.)?(?:youtube\\.com\\/(?:[^\\/\\n\\s]+\\/\\S+\\/|(?:v|e(?:mbed)?)\\/|.*[?&]v=)|youtu\\.be\\/)([a-zA-Z0-9_-]{11})\"\n",
    "    match = re.search(regex, youtube_url)\n",
    "    return match.group(1) if match else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_transcript_yt_api(video_id):\n",
    "    \"\"\"\n",
    "    Gets the transcript of a youtube video\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    video_id : str\n",
    "        The video id\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The transcript of the video\n",
    "    \"\"\"\n",
    "    try:\n",
    "        full_transcript = \" \"\n",
    "        transcript = YouTubeTranscriptApi.get_transcript(video_id)\n",
    "        for line in transcript:\n",
    "            full_transcript += line['text'] + \" \"\n",
    "        return full_transcript\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\" let's get started with our linear algebra review in this video i want to tell you what are matrices and what are vectors a matrix is a rectangular array of numbers written between square brackets so for example here is a matrix i'm going to write a left square bracket and then write in a bunch of numbers and you know these could be features for a machine learning problem or it could be data from somewhere else but for example the specific values don't matter and then i'm gonna close it with another right bracket on the right so that's one matrix and you know here's another example of a matrix mr right one two three four five six so matrix is just another way for saying is a 2d or two dimensional array and the other piece of analogy we need is that the dimensional matrix is going to be written as the number of rows times the number of columns in the matrix so concretely this example on the left this has one two three four rows and it has two columns and so this example on the left i'm going to say this is a four by two matrix okay because it's number of rows by number of columns so four rows two columns this one on the right this matrix has two rows so that's the first row that's the second row and it has three columns right that's the first column that's the second column that's the third column so this second matrix we say it is a two by three matrix so we say that the dimension of this matrix is two by three and sometimes you also see this written out as uh in the case of the left you see this written out as our four by two or concretely what people sometimes say is that this matrix is an element of the set r four by two so this this thing here this just means a set of all matrices that are of dimension four by two and this thing on the right sometimes it's written out as a matrix that's an r two by three so if you ever see excuse me two by three so if you ever see something like this you know alpha by two or r two by three people are just referring to matrices of a specific dimension next let's talk about how to refer to specific elements of the matrix and by matrix elements i love the matrix i just mean the entries or the numbers inside the matrix so in the standard notation if a is this matrix here then a substring i j is going to refer to the i comma j entry meaning the entry in the matrix is in the i've row and j column so for example a11 is we're going to refer to the entry in the first row and the first column so that's the first row in the first column and so you know a11 is going to be equal to 1402 another example a12 is going to refer to the entry in the first row and the second column and so a12 is going to be equal to 191 just a couple more quick examples let's see a oh let's say a32 is going to refer to the entry in the third row and the second column right because that's three two so that's equal to one four three seven and finally a four one is going to refer to you know this one right fourth row first column is equal to one four seven and if hopefully you won't but if you were to write say what is a four three well that refers to the fourth row and the third column but you know this matrix has no third column so this is undefined you know or you can think of this as an error so there's no such element as a a43 so you know shouldn't be referring to a43 so the matrix gives you a way of letting you quickly organize index and access lots of data and in case i seem to be tossing up a lot of concepts a lot of new notation very rapidly you don't need to memorize all this but on the course website where we have posted the lecture notes we also have all of these definitions written down so you can always refer back you know either to these slides post on the course website or to the lecture notes if you forget like well a41 is that which row which column is that don't worry about memorizing everything now you can always refer back to the written materials on the course website use that as a reference so that's what the matrix is next let's talk about what is a vector a vector turns out to be a special case of a matrix a vector is a matrix that has only one column so if you have an n by one matrix then that's a remember right n is the number of rows and one here is number of columns so so a matrix with just one column is what we call a vector so here's an example of a vector right with uh i guess i have n equals four elements here so we also called this thing another term for this is that this is a four dimensional vector just means that this is a vector with four elements with four numbers in it and just as you know earlier for matrices you saw this notation that r 3 by 2 is referred to 3 by 2 matrices right for this vector we're going to refer this to to this as a vector in the set's r4 so this r4 again just means the set of all four dimensional vectors next let's talk about how to refer to elements of the vector we're going to use the notation y i to refer to the element of the vector y so why is this vector y subscript i is the i element so y one is the first element is equal to 460 you know y2 is equal to the second element right 232 that's the first there's a second y3 is equal to 315 and so on and only y1 through y4 are defined because this is a four-dimensional vector also it turns out that there are actually two conventions for how to index into a vector and here they are sometimes people will use one index and sometimes zero index vectors so this example on the left is a one index vector where the elements we write as y one y two y three y four and this example on the right is an example of a zero index vector where we start the indexing of the elements from zero so the elements go from y zero up to y three and this is a little bit like the arrays of some programming languages right where the arrays can either be indexed starting from one so the first element of an array sometimes say y1 that's kind of c plus notation i guess and sometimes you know it's zero zero index depending on what programming language you use so it turns out that in most of math the one index version is more common but for a lot of machine learning applications zero index vectors gives us a more convenient notation so what you should usually do is uh unless otherwise specified you should assume that we're using one index vectors in fact throughout the rest of these videos on linear algebra review i will be using one index vectors but just be aware that when we talk about machine learning applications sometimes you know i'll explicitly say when we need to switch to when we need to use zero index vectors as well finally by convention usually when writing matrices and vectors most people will use uppercase to refer to matrices so we use capital matrix capital letters like a b c you know x to refer to matrices and usually we'll use lowercase like a b x y to refer to either numbers just real numbers or scalars or two vectors but this isn't always true but this is the more common notation where we use the lowercase y to refer to vector and we usually use uppercase to refer to a matrix so you now know what are matrices and vectors next we'll talk about some of the things you can do with them \""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "video_id = extract_video_id(\"https://www.youtube.com/watch?v=7wpfu30FYJM&list=PL2qEL_7r0QISg3wu4D_j9xRJodZsfjBEu\")\n",
    "transcript = get_transcript_yt_api(video_id)\n",
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_client = boto3.client(\"s3\")\n",
    "BUCKET_NAME = \"tubequiz-bucket\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_audio(link):\n",
    "    video_id = extract_video_id(link)\n",
    "    output_path = f\"{video_id}_audio\"\n",
    "    ydl_opts = {\n",
    "        'format':'bestaudio/best',\n",
    "        'outtmpl':output_path,\n",
    "        'postprocessors':[{\n",
    "            'key':'FFmpegExtractAudio',\n",
    "            'preferredcodec':'mp3',\n",
    "            'preferredquality':'192'\n",
    "        }],\n",
    "\n",
    "    }\n",
    "    \n",
    "    with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
    "        ydl.download([link])\n",
    "\n",
    "    return output_path+'.mp3'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_s3(file_path):\n",
    "    s3_key = f\"audio/{file_path}\"\n",
    "    s3_client.upload_file(file_path, BUCKET_NAME, s3_key)\n",
    "    return f\"s3://{BUCKET_NAME}/{s3_key}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcribe_client = boto3.client(\"transcribe\", region_name=\"us-west-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transcribe_audio(s3_uri, job_name):\n",
    "\n",
    "    \"\"\"Transcribes an audio file using AWS Transcribe.\"\"\"\n",
    "    transcribe_client.start_transcription_job(\n",
    "        TranscriptionJobName=job_name,\n",
    "        Media={'MediaFileUri': s3_uri},\n",
    "        MediaFormat='mp3',\n",
    "        LanguageCode='en-US'\n",
    "    )\n",
    "\n",
    "    # Wait for the job to complete\n",
    "    while True:\n",
    "        status = transcribe_client.get_transcription_job(TranscriptionJobName=job_name)\n",
    "        if status['TranscriptionJob']['TranscriptionJobStatus'] in ['COMPLETED', 'FAILED']:\n",
    "            break\n",
    "\n",
    "    if status['TranscriptionJob']['TranscriptionJobStatus'] == 'COMPLETED':\n",
    "        transcript_uri = status['TranscriptionJob']['Transcript']['TranscriptFileUri']\n",
    "        return transcript_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[youtube] Extracting URL: https://youtu.be/6M5VXKLf4D4?si=GW_-1F3kF-7Kr5zX\n",
      "[youtube] 6M5VXKLf4D4: Downloading webpage\n",
      "[youtube] 6M5VXKLf4D4: Downloading tv client config\n",
      "[youtube] 6M5VXKLf4D4: Downloading player 7d1d50a6\n",
      "[youtube] 6M5VXKLf4D4: Downloading tv player API JSON\n",
      "[youtube] 6M5VXKLf4D4: Downloading ios player API JSON\n",
      "[youtube] 6M5VXKLf4D4: Downloading m3u8 information\n",
      "[info] 6M5VXKLf4D4: Downloading 1 format(s): 251\n",
      "[download] Destination: 6M5VXKLf4D4_audio\n",
      "[download] 100% of    5.75MiB in 00:00:01 at 4.62MiB/s   \n",
      "[ExtractAudio] Destination: 6M5VXKLf4D4_audio.mp3\n",
      "Deleting original file 6M5VXKLf4D4_audio (pass -k to keep)\n"
     ]
    }
   ],
   "source": [
    "audio_path = download_audio(\"https://youtu.be/6M5VXKLf4D4?si=GW_-1F3kF-7Kr5zX\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'6M5VXKLf4D4_audio.mp3'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NoCredentialsError",
     "evalue": "Unable to locate credentials",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNoCredentialsError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m s3_video_uri \u001b[38;5;241m=\u001b[39m upload_s3(audio_path)\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36mupload_s3\u001b[0;34m(file_path)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupload_s3\u001b[39m(file_path):\n\u001b[1;32m      2\u001b[0m     s3_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maudio/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 3\u001b[0m     s3_client\u001b[38;5;241m.\u001b[39mupload_file(file_path, BUCKET_NAME, s3_key)\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mBUCKET_NAME\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00ms3_key\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/botocore/context.py:124\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[1;32m    123\u001b[0m     hook()\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/boto3/s3/inject.py:170\u001b[0m, in \u001b[0;36mupload_file\u001b[0;34m(self, Filename, Bucket, Key, ExtraArgs, Callback, Config)\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Upload a file to an S3 object.\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03mUsage::\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    transfer.\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m S3Transfer(\u001b[38;5;28mself\u001b[39m, Config) \u001b[38;5;28;01mas\u001b[39;00m transfer:\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m transfer\u001b[38;5;241m.\u001b[39mupload_file(\n\u001b[1;32m    171\u001b[0m         filename\u001b[38;5;241m=\u001b[39mFilename,\n\u001b[1;32m    172\u001b[0m         bucket\u001b[38;5;241m=\u001b[39mBucket,\n\u001b[1;32m    173\u001b[0m         key\u001b[38;5;241m=\u001b[39mKey,\n\u001b[1;32m    174\u001b[0m         extra_args\u001b[38;5;241m=\u001b[39mExtraArgs,\n\u001b[1;32m    175\u001b[0m         callback\u001b[38;5;241m=\u001b[39mCallback,\n\u001b[1;32m    176\u001b[0m     )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/boto3/s3/transfer.py:372\u001b[0m, in \u001b[0;36mS3Transfer.upload_file\u001b[0;34m(self, filename, bucket, key, callback, extra_args)\u001b[0m\n\u001b[1;32m    368\u001b[0m future \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_manager\u001b[38;5;241m.\u001b[39mupload(\n\u001b[1;32m    369\u001b[0m     filename, bucket, key, extra_args, subscribers\n\u001b[1;32m    370\u001b[0m )\n\u001b[1;32m    371\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 372\u001b[0m     future\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# If a client error was raised, add the backwards compatibility layer\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# that raises a S3UploadFailedError. These specific errors were only\u001b[39;00m\n\u001b[1;32m    375\u001b[0m \u001b[38;5;66;03m# ever thrown for upload_parts but now can be thrown for any related\u001b[39;00m\n\u001b[1;32m    376\u001b[0m \u001b[38;5;66;03m# client error.\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ClientError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/s3transfer/futures.py:111\u001b[0m, in \u001b[0;36mTransferFuture.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mresult\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    107\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    108\u001b[0m         \u001b[38;5;66;03m# Usually the result() method blocks until the transfer is done,\u001b[39;00m\n\u001b[1;32m    109\u001b[0m         \u001b[38;5;66;03m# however if a KeyboardInterrupt is raised we want want to exit\u001b[39;00m\n\u001b[1;32m    110\u001b[0m         \u001b[38;5;66;03m# out of this and propagate the exception.\u001b[39;00m\n\u001b[0;32m--> 111\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_coordinator\u001b[38;5;241m.\u001b[39mresult()\n\u001b[1;32m    112\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    113\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel()\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/s3transfer/futures.py:272\u001b[0m, in \u001b[0;36mTransferCoordinator.result\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    269\u001b[0m \u001b[38;5;66;03m# Once done waiting, raise an exception if present or return the\u001b[39;00m\n\u001b[1;32m    270\u001b[0m \u001b[38;5;66;03m# final result.\u001b[39;00m\n\u001b[1;32m    271\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m--> 272\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[1;32m    273\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_result\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/s3transfer/tasks.py:142\u001b[0m, in \u001b[0;36mTask.__call__\u001b[0;34m(self, ctx)\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[38;5;66;03m# If the task is not done (really only if some other related\u001b[39;00m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;66;03m# task to the TransferFuture had failed) then execute the task's\u001b[39;00m\n\u001b[1;32m    140\u001b[0m     \u001b[38;5;66;03m# main() method.\u001b[39;00m\n\u001b[1;32m    141\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_coordinator\u001b[38;5;241m.\u001b[39mdone():\n\u001b[0;32m--> 142\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execute_main(kwargs)\n\u001b[1;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_log_and_set_exception(e)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/s3transfer/tasks.py:165\u001b[0m, in \u001b[0;36mTask._execute_main\u001b[0;34m(self, kwargs)\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[38;5;66;03m# Log what is about to be executed.\u001b[39;00m\n\u001b[1;32m    163\u001b[0m logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExecuting task \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with kwargs \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkwargs_to_display\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 165\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_main(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    166\u001b[0m \u001b[38;5;66;03m# If the task is the final task, then set the TransferFuture's\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[38;5;66;03m# value to the return value from main().\u001b[39;00m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_is_final:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/s3transfer/tasks.py:351\u001b[0m, in \u001b[0;36mCreateMultipartUploadTask._main\u001b[0;34m(self, client, bucket, key, extra_args)\u001b[0m\n\u001b[1;32m    341\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    342\u001b[0m \u001b[38;5;124;03m:param client: The client to use when calling CreateMultipartUpload\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[38;5;124;03m:param bucket: The name of the bucket to upload to\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;124;03m:returns: The upload id of the multipart upload\u001b[39;00m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# Create the multipart upload.\u001b[39;00m\n\u001b[0;32m--> 351\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mcreate_multipart_upload(\n\u001b[1;32m    352\u001b[0m     Bucket\u001b[38;5;241m=\u001b[39mbucket, Key\u001b[38;5;241m=\u001b[39mkey, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mextra_args\n\u001b[1;32m    353\u001b[0m )\n\u001b[1;32m    354\u001b[0m upload_id \u001b[38;5;241m=\u001b[39m response[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mUploadId\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    356\u001b[0m \u001b[38;5;66;03m# Add a cleanup if the multipart upload fails at any point.\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/botocore/client.py:570\u001b[0m, in \u001b[0;36mClientCreator._create_api_method.<locals>._api_call\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    566\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[1;32m    567\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpy_operation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m() only accepts keyword arguments.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    568\u001b[0m     )\n\u001b[1;32m    569\u001b[0m \u001b[38;5;66;03m# The \"self\" in this scope is referring to the BaseClient.\u001b[39;00m\n\u001b[0;32m--> 570\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_api_call(operation_name, kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/botocore/context.py:124\u001b[0m, in \u001b[0;36mwith_current_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m hook:\n\u001b[1;32m    123\u001b[0m     hook()\n\u001b[0;32m--> 124\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/botocore/client.py:1013\u001b[0m, in \u001b[0;36mBaseClient._make_api_call\u001b[0;34m(self, operation_name, api_params)\u001b[0m\n\u001b[1;32m   1009\u001b[0m     maybe_compress_request(\n\u001b[1;32m   1010\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mconfig, request_dict, operation_model\n\u001b[1;32m   1011\u001b[0m     )\n\u001b[1;32m   1012\u001b[0m     apply_request_checksum(request_dict)\n\u001b[0;32m-> 1013\u001b[0m     http, parsed_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_request(\n\u001b[1;32m   1014\u001b[0m         operation_model, request_dict, request_context\n\u001b[1;32m   1015\u001b[0m     )\n\u001b[1;32m   1017\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m   1018\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1019\u001b[0m     http_response\u001b[38;5;241m=\u001b[39mhttp,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1025\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http\u001b[38;5;241m.\u001b[39mstatus_code \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m300\u001b[39m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/botocore/client.py:1037\u001b[0m, in \u001b[0;36mBaseClient._make_request\u001b[0;34m(self, operation_model, request_dict, request_context)\u001b[0m\n\u001b[1;32m   1035\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_make_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict, request_context):\n\u001b[1;32m   1036\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1037\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_endpoint\u001b[38;5;241m.\u001b[39mmake_request(operation_model, request_dict)\n\u001b[1;32m   1038\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1039\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmeta\u001b[38;5;241m.\u001b[39mevents\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m   1040\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mafter-call-error.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_service_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_model\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   1041\u001b[0m             exception\u001b[38;5;241m=\u001b[39me,\n\u001b[1;32m   1042\u001b[0m             context\u001b[38;5;241m=\u001b[39mrequest_context,\n\u001b[1;32m   1043\u001b[0m         )\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/botocore/endpoint.py:119\u001b[0m, in \u001b[0;36mEndpoint.make_request\u001b[0;34m(self, operation_model, request_dict)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmake_request\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_model, request_dict):\n\u001b[1;32m    114\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMaking request for \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m with params: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    116\u001b[0m         operation_model,\n\u001b[1;32m    117\u001b[0m         request_dict,\n\u001b[1;32m    118\u001b[0m     )\n\u001b[0;32m--> 119\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_send_request(request_dict, operation_model)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/botocore/endpoint.py:196\u001b[0m, in \u001b[0;36mEndpoint._send_request\u001b[0;34m(self, request_dict, operation_model)\u001b[0m\n\u001b[1;32m    194\u001b[0m context \u001b[38;5;241m=\u001b[39m request_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_retries_context(context, attempts)\n\u001b[0;32m--> 196\u001b[0m request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_request(request_dict, operation_model)\n\u001b[1;32m    197\u001b[0m success_response, exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_response(\n\u001b[1;32m    198\u001b[0m     request, operation_model, context\n\u001b[1;32m    199\u001b[0m )\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_needs_retry(\n\u001b[1;32m    201\u001b[0m     attempts,\n\u001b[1;32m    202\u001b[0m     operation_model,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m     exception,\n\u001b[1;32m    206\u001b[0m ):\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/botocore/endpoint.py:132\u001b[0m, in \u001b[0;36mEndpoint.create_request\u001b[0;34m(self, params, operation_model)\u001b[0m\n\u001b[1;32m    130\u001b[0m     service_id \u001b[38;5;241m=\u001b[39m operation_model\u001b[38;5;241m.\u001b[39mservice_model\u001b[38;5;241m.\u001b[39mservice_id\u001b[38;5;241m.\u001b[39mhyphenize()\n\u001b[1;32m    131\u001b[0m     event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrequest-created.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mservice_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation_model\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 132\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_event_emitter\u001b[38;5;241m.\u001b[39memit(\n\u001b[1;32m    133\u001b[0m         event_name,\n\u001b[1;32m    134\u001b[0m         request\u001b[38;5;241m=\u001b[39mrequest,\n\u001b[1;32m    135\u001b[0m         operation_name\u001b[38;5;241m=\u001b[39moperation_model\u001b[38;5;241m.\u001b[39mname,\n\u001b[1;32m    136\u001b[0m     )\n\u001b[1;32m    137\u001b[0m prepared_request \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_request(request)\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m prepared_request\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/botocore/hooks.py:412\u001b[0m, in \u001b[0;36mEventAliaser.emit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    410\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    411\u001b[0m     aliased_event_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_alias_event_name(event_name)\n\u001b[0;32m--> 412\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_emitter\u001b[38;5;241m.\u001b[39memit(aliased_event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/botocore/hooks.py:256\u001b[0m, in \u001b[0;36mHierarchicalEmitter.emit\u001b[0;34m(self, event_name, **kwargs)\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21memit\u001b[39m(\u001b[38;5;28mself\u001b[39m, event_name, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    246\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    Emit an event by name with arguments passed as keyword args.\u001b[39;00m\n\u001b[1;32m    248\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;124;03m             handlers.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_emit(event_name, kwargs)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/botocore/hooks.py:239\u001b[0m, in \u001b[0;36mHierarchicalEmitter._emit\u001b[0;34m(self, event_name, kwargs, stop_on_response)\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m handler \u001b[38;5;129;01min\u001b[39;00m handlers_to_call:\n\u001b[1;32m    238\u001b[0m     logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m: calling handler \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m'\u001b[39m, event_name, handler)\n\u001b[0;32m--> 239\u001b[0m     response \u001b[38;5;241m=\u001b[39m handler(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    240\u001b[0m     responses\u001b[38;5;241m.\u001b[39mappend((handler, response))\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m stop_on_response \u001b[38;5;129;01mand\u001b[39;00m response \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/botocore/signers.py:106\u001b[0m, in \u001b[0;36mRequestSigner.handler\u001b[0;34m(self, operation_name, request, **kwargs)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mhandler\u001b[39m(\u001b[38;5;28mself\u001b[39m, operation_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    102\u001b[0m     \u001b[38;5;66;03m# This is typically hooked up to the \"request-created\" event\u001b[39;00m\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;66;03m# from a client's event emitter.  When a new request is created\u001b[39;00m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;66;03m# this method is invoked to sign the request.\u001b[39;00m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;66;03m# Don't call this method directly.\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msign(operation_name, request)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/botocore/signers.py:198\u001b[0m, in \u001b[0;36mRequestSigner.sign\u001b[0;34m(self, operation_name, request, region_name, signing_type, expires_in, signing_name)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    196\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[0;32m--> 198\u001b[0m auth\u001b[38;5;241m.\u001b[39madd_auth(request)\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/botocore/auth.py:424\u001b[0m, in \u001b[0;36mSigV4Auth.add_auth\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21madd_auth\u001b[39m(\u001b[38;5;28mself\u001b[39m, request):\n\u001b[1;32m    423\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcredentials \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 424\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m NoCredentialsError()\n\u001b[1;32m    425\u001b[0m     datetime_now \u001b[38;5;241m=\u001b[39m datetime\u001b[38;5;241m.\u001b[39mdatetime\u001b[38;5;241m.\u001b[39mutcnow()\n\u001b[1;32m    426\u001b[0m     request\u001b[38;5;241m.\u001b[39mcontext[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m datetime_now\u001b[38;5;241m.\u001b[39mstrftime(SIGV4_TIMESTAMP)\n",
      "\u001b[0;31mNoCredentialsError\u001b[0m: Unable to locate credentials"
     ]
    }
   ],
   "source": [
    "s3_video_uri = upload_s3(audio_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "video_id = extract_video_id(\"https://www.youtube.com/watch?v=MfIjxPh6Pys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_video_uri  = transcribe_audio(s3_video_uri, f\"transcription-{video_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://s3.us-west-2.amazonaws.com/aws-transcribe-us-west-2-prod/841162666835/transcription-MfIjxPh6Pys/76330c46-1510-47e8-8f6a-cedc774f2e6a/asrOutput.json?X-Amz-Security-Token=IQoJb3JpZ2luX2VjEN3%2F%2F%2F%2F%2F%2F%2F%2F%2F%2FwEaCXVzLXdlc3QtMiJIMEYCIQChryXD8bNaSy6o2TlJQ3IN2l3wUlwUcLUDewdglb5ZHQIhAJIxVqSLyIxOyApSV95vB%2BAqJTe2GZy54zNn9NjIDRIiKrEFCDYQBBoMMDgwMjQ4MzIyMjA2Igw6VpBlJqlTMuHRm%2BMqjgW2jLNKc%2F%2Bs5FtmedK5a7d9%2FTdLr4pkLAlRliy%2F17Gc%2F490bIAGzL72%2BfaJ3mhb0I0PtyT8Rkyf%2B4dGAAYa1CtP%2FVvubeVnpoSHUbGeLz4FZlA%2FYZ2OU2XjQ18wwyY75Z1N9fKQmavM%2FSZ5%2Fx5uFY1VsMqhs0C6hWpscqHWfHvxaxmkA9SCzBQLUkb2ufOL%2BOC7RE%2FfQurLYYhhvP5DVo7dRzMEWGARmnpdsJA%2FdAqZ2wmDKmaRxZkCO%2F1Jh0ZherviXYrZHEKUEplTWMwK%2FM8snU2yoWpmbteFzMVA0Xbu9SUZW8PmoqLbJL%2F%2F%2BNeZBMEuxPJRs0dcadkXSI9FJ5f1CeaBy4iodmmVRs0nl545elQaVKBhjvl%2BbPGDc64IaFEJSoWdPd%2B%2Fo0mTwGlzkqTVYQNleFJFiICb2RmoPnKr7qfNndE7hsl8RACDUb%2BHSjrAX1vmKkg3g60kR4dYWBAEBsGwH0w%2FdfsyWlMjnO4Wjl44bos2eJrVqJDvGYeRg1Q0dcxsRLmCWYrOsntuiiVvLOzUYQh5%2BX%2FWUCzoby3Se9sxBZNUWxEXRLSSxiMU1lL1KDGr7jEqxV47u0ZK23EIs%2BwIQaVAMHl2B1ZzjyppXrgYNHmIdeREf2fZBaL8VvN%2FQBbhxffxSlUGLuZHyDntz455SiRfmFkIJzoTHTHY%2FiLZzJ9LL3hpmdfytYNhyyGSbClbOvNrvh2BbjupkEawts7shK9IZf0QiI36b2xOvKexoQu%2FLmZysj%2BKjtIV5TDIyEwh2MgBMP0nTrovPHOF%2FD0sA3itHTTWtegMuUuT4cQ9%2FBzOP%2FACpZrVRCopow9V3XUEmwbSe8liKGfg%2FCg0sNwwPGL721busYtAaekw1vXcvgY6sAGKkNIhC2VY%2BNlLgZcOTEKm7qJVHD7BUsr4EF1dhuBCXvNZ8bt7Xc7%2Fq5m2FP8I9A%2FG98gCDFa1Ex1x%2FYUO0LVTiJ%2BUOKijeFTVALxle13%2FPVEaidOcZV%2BuBKFZYauNAGznLvTE%2FJjTPbPlFeK3t8yATpS4ninZ76GPbQ8Vo5K7aihJYm72IS4LmeatL%2F7OC6iXzCh880HVMoh%2BjOu1rA62LV5K3SYXU3IEAT64WFz7AA%3D%3D&X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Date=20250316T220537Z&X-Amz-SignedHeaders=host&X-Amz-Expires=900&X-Amz-Credential=ASIARFLZMHCPEHMWA5RQ%2F20250316%2Fus-west-2%2Fs3%2Faws4_request&X-Amz-Signature=dd8b46095aceab995d98a066343276aeb50c81a812c58d0df068593d13afd3ca'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript_video_uri"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript_data = requests.get(transcript_video_uri).json()\n",
    "transcript = transcript_data['results']['transcripts'][0]['transcript']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hello everyone, uh, welcome to CS 229. Um, today we're going to talk about, uh, deep learning and neural networks. Um, we're going to have two lectures on that one today and a little bit more of it on, uh, Monday. Um, don't hesitate to ask questions during the lecture, uh, so stop me if you don't understand something, and we'll try to build the intuition around your own network together. We will actually start with an algorithm that you guys have seen, uh, previously called logistic regression. Everybody remembers logistic regression. OK, remember it's a classification algorithm. Um, we're going to do that, explain how logistic regression can be interpreted as a neural network specific case of the neural network. And then we will go to neural networks. Sounds good? So the quick intro on deep learning. So deep learning is a is a set of techniques that is let's say a subset of machine learning and it's one of the growing techniques that have been used in the industry specifically for problems in computer vision, natural language processing, and speech recognition so you guys have a lot of different tools and and uh plugins on your smartphones that uses this type of algorithm. Uh, the reason it came. Uh, to work very well is primarily the the new computational methods. So one thing we're going to see today um is that deep learning is really really computationally expensive and where people had to find techniques in order to parallelize the code and use GPUs specifically in order to graphical processing units in order to be able to compute uh the the the the computations in deep learning. Uh, the second part is the data. Available Has been growing after. After the Internet bubble, the the digitalization of the world, so now people have access to large amounts of data and this type of algorithm has the specificity of being able to learn a lot when there's a lot of data. So these models are very flexible, and the more you give them data, the more they will be able to understand the salient feature of the data. And finally, algorithms. So people have come up with with new techniques uh in order to use the data use the competition power and build models so we're going to touch a little bit on all of that but let's go with logistic res first. Can you guys see in the back? Yeah, OK, perfect. So, You remember uh what logistical question is we're we're going to fix a goal for us uh that uh is a classification goal. So let's try to to find cats in images so find. Cuts in images. Meaning binary classification, if there is a cat in the image. We want to output a number that is close to one presence of a cat. And if there is no cut in the image. We want output 0. Let, let's say for now uh we're constrained to the fact that there is maximum 1 cat per image. There's no more. If you had to draw the logistic regression model that's what you would do. You would take a cat? So this is an image of a cat? Very bad at that. Um, Sorry. In computer science, you know that images can be represented as 3D matrices. So if I tell you that this is a color image of size 64 by 64. How many numbers do I have to represent those pixels? Yeah, I heard it. 64 by 64 by 33 for the RGB channel. Red Green, blue. Every pixel in an image can be represented by three numbers one representing the red filter, the green filter, and the and the blue filter. So actually this image is of size 64 times 64 times 3. That makes sense? So the first thing we will do in order to use logistic regression to find if there is a cut on this image, we're going to flatten this into a vector. So I'm going to take all the numbers in this matrix and flatten them in a vector. Just an image to vector operation. Nothing more. And now I can use my logistic regression because I have a vector input. So I'm going to To take all of these and push them in an operation that we call the logistic operation which has one part that is. W X plus B or X? Is going to be the image? So WX plus B and the second part is going to be the sigmoid. Everybody's familiar with the sigmoid function function that takes a number between minus infinity and plus infinity maps it between 0 and 1. It's very convenient for classification problems, and this we're going to call it Y hat, which is sigmoid of what you've seen in class previously, I think it's data transpose X, but here we will just separate the notation into W and B. So can someone tell me what's the shape of W? The Matrix W. Vector matrix. Hm? What? Yes, 64 by 64 by 3 as a. Yeah, so you know that this guy here? Is a vector of 64 by 64 by 3, a column vector, so the shape of X is going to be 64 by 64 by 3. Times one, this is the shape and this I think it's. That if I don't know 12 12,288. And uh this indeed because we want Y hats to be one by one this W has to be one. By 12 288 that makes sense? So we have a row vector as our parameter. We're just changing the notations of the logistic regression that you guys have seen and so once we have this model we need to train it as you know and the process of training is that first we will initialize. Or parameters, these are what we call parameters. We will use the specific vocabulary of weights. And bias? I believe you guys have heard this vocabulary before weights and biases. So we're going to find the right W and the right B in order to be able uh to use this model properly once we initialize them, what we will do is that we will optimize them. Find The optimal W and B and after we found the optimal WNB we will use them to predict. Does this process make sense, this training process? And I think the important part is to understand what this is. Find the optimal WNB means defining a loss function, which is the objective and in machine learning you often have this this this specific problem where you have a function that you know you want to find the network function but you don't know the values of its parameters in order to find them you're going to use a proxy that is going to be your last function. If you manage to minimize the loss function, you will find the right parameters. So you defined a loss function. That is the logistic loss. Y log of Y hat plus 1 minus Y log of 1 minus Y hat? Up Uh, you guys have seen this one. You remember where it comes from? Comes from a maximum likelihood estimation starting from a probabilistic model. And so the idea is how can I minimize this function? Minimize because I've put a minus sign here. I want to find W and B that minimize this function and I'm going to use a gradient descent algorithm. Which means I'm going to iteratively. Compute the derivative of the loss with respect to my parameters? And at every step I will update them to make this loss function go a little down at every iterative step. So in terms of implementation, this is a 4 loop. You will loop over a certain number of iteration. And at every point you will compute the derivative of your loss with respect to your parameters. Everybody remembers how to compute this number. Take the derivative here, you use the fact that the sigmoid function has a derivative that is sigmoid times 1 minus sigmoid. And you will compute the results we're we're going to do some derivative later today. But just to to set up the problem here, so the, the few things that I wanna that I wanna touch on here is first how many parameters does this model have this logistic regression if you have to count them. So this is the number 89, yeah, correct. So 12,288 weights and 1 bias that makes sense? So actually it's funny because you can quickly count it by just counting the number of edges on the on the on the drawing plus 1. Every circle has a bias, every edge has a weight because ultimately this operation, you can rewrite it like that, right? It means every weight has. Every weight corresponds to an edge, so that's another way to count it. We're going to use it a little further. So we're starting with not too many parameters actually, and one thing that we noticed is that the number of parameters of the model depends on the size of the inputs. We probably don't want that at some point, so we're going to change it. Later on. So two equations that I want you to remember is the first one is neuron equals linear. Plus activation. So this is the vocabulary we will use in neural networks. We define a neuron as an operation that has two parts one linear part and one activation part, and it's exactly that. This is actually a neuron. We have a linear part WX plus B, and then we take the output of this linear part and we put it in an activation that in this case is the sigmoid function. It can be other functions. OK, so this is the first equation. Not too hard. The second equation that I want to set now is the model. Equals Architecture Plus parameters. What does that mean? It means here we're we're trying to train a logistic regression in order to to be able to use it we need an architecture which is the following a one neuron neuron network. And the parameters W and B. So basically when people say uh we've shipped the model. Like in the industry, what they're saying is that they found the right parameters with the right architecture. They have two files and these two files are predicting a bunch of things. OK. One parameter file and one architecture file. The architecture will be modified a lot today. We will add neurons all over, uh, and the parameters will always be called W and B, but they will become bigger and bigger because we have more data we wanna be able to understand it. You can get that it's going to be hard to understand what a cat is with only that that that many parameters we want to have more parameters. Any questions so far? So this was just to set up the problem with logistic regression. Let's try to set a new goal after the first goal we have. Set prior to that. So the second goal would be find. Cats A lion Iguana In images. So a little different than before, only thing we changed is that we want now to detect 3 types of animals. If there's a cat on the image, I wanna know there's a cat. If there's an iguana on the image, I wanna know there's an iguana. If there's a line on the image, I wanna know it as well. So how would you modify the network that we previously had in order to take this into account? Yeah. Yeah, good idea. So put 2 more circles, so neurons and do the same thing. So we have our picture here. With the cats? So the cat is going to the right. It 64 by 64 by 3 we flatten it from X1 to XN. Let's say N represents 6464 by 3. And what I will do is that I will use 3 neurons. That are all computing the same thing they're all connected to all these inputs. OK, I connect all my inputs X1 to XN to each of these neurons. And I will use a specific set of notation here. OK. Y2 hats equals A21 sigmoid of W 21 + X + B 21. And similarly Y3 hats equals 831? Which is sigmoid of W31 X plus B31. So I'm introducing a few notations here and we we get used to it don't worry so just just write this down and we're going to go over it. So the square brackets here. Represents what we will call later on a layer. If you look at this network, it looks like there is one layer here. There's one layer in which neurons don't communicate with each other. We could add up to it and we will do it later on more neurons in other layers. We will denote with square brackets the index of the layer. The index that is the subscript to this A is the number identifying the neuron inside the layer. So here we have one layer, we have A1, A2 and A3 with square brackets 1 to identify the layer. Does that make sense? And then we have our white hats that instead of being a single number as it was before is now a vector of size 3. So how many parameters does this network have? How much? OK, how did you come up with that? OK, yeah, correct, so we just have 3 times the, the thing we had before because we added 2 more neurons and they all have their own set of parameters. Look like this edge is a separate edge of this one, so we, we have to replicate parameters for each of these. So W11 would be the equivalent of what we had for the cat, but we have to add two more, uh. parameter vectors and biases. So other question when you had to train this logistic regression, what data set did you need? Did someone try to describe the data set. Yeah. You Yeah, correct. So we need images and labels with it labeled as cat 1 or no cat 0. So it's a binary classification with images and labels. Now, what do you think should be the data set to train this network? Yes, go for it. That's a good idea. So just to repeat, uh, a label for an image that has a cat. Would probably be a vector. With a 1 and 20s, where the one should represent the the presence of a cat. This one should represent the presence of a lion, and this one should represent the presence of an iguana. So let's assume I use this scheme to label my data set. I train this network using the same techniques here, initialize all my weights and biases with a value. A starting value optimize a loss function. By using gradient descent and then use Y equals la la la to predict. What do you think this neuron is going to be responsible for? If you had to describe the responsibility of this neuron. Yes. Well, this one. Yeah, lion and this one iguana. So basically the the the way you yeah go for it. That's a good question. We're going to talk about that now. Multiple image contain different animals or not. So going back on what you said because we decided to label our data set like that after training, this neuron is naturally going to be there to detect cats. If we had changed the labeling scheme and I said that the second entry would correspond to the cat, the presence of the cat, then after training you will detect that this neuron is responsible for detecting the cat, so the network is going to evolve depending on the way you label your data set. Now do you think that this network can still be robust to different animals in the same picture? So this cat now has a a friend that is a lion. OK, I have no idea how to draw a lion, but let's say there's a lion here. And because there's a lion, I will add a one here. Do you think this network is robust to this type of labeling? Hm. It should be the neurons aren't talking to each other. That's a good answer actually another answer. Um That's a good intuition because the network what it sees is just 110 and an image. It doesn't see that this one correspond to the cat correspond to the first one and the second and the line correspond to the second one, so this is a property of neural networks. It's the fact that you don't need to tell them everything. If you have enough data, they're going to figure it out. So because you will have also cats with iguanas, cats alone, lions with iguanas, lions alone, ultimately this neuron will understand what it's looking for and it will understand that this one corresponds to this lion. Just needs a lot of data. So yes it's going to be robust and that's the reason you mentioned it's going to be robust to that because the three neurons aren't communicating together. So we can totally train them independent independently from each other and in fact the sigmoid here doesn't depend on the sigmoid here and doesn't depend on the sigmoid here it means we can have 11 and one as an output. Yes, question. You could, you could, you could think about it as 3 logistic regressions, so we wouldn't call that a neural network yet. It's not ready yet, but it's a. 3 neuron neural network or 3 logistical aggression with each other. Now, following up on that, uh, yeah, go forward to question. W and B are related to what? Oh yeah, yeah. So, so usually you would have theta transposed X, which is sum of theta I XI, correct? And what I will split it is I will split it in sum of theta I XI plus theta 0. Times one I'll split it like that. The 0 would correspond to B and this data Is would correspond to WIs. Makes sense. OK. One more question and then we move on. Good question. That's the next thing we're going to see. So the question is a follow up on this is there cases where we have a constraint where there is only one possible outcome? It means there is no cat and lion there's either a cat or a lion. There's no iguana and lion there's either an iguana or a lion. Think about health care. There are many, there are many. Models that are made to detect uh if if a disease skin disease is present on based on cell microscopic images. Usually there is no overlap between this disease. It means you want to classify a specific disease among a large number of diseases. So this model would still work but would not be optimal because it's longer to train. Maybe one disease is super, super rare and one of the neurons is never going to be trained. Let's say you're working in a zoo where there is only one iguana and there are thousands of lions and thousands of cats. This guy will never train almost you know it would be super hard to train this one so you wanna start with another model that where you put the constraint that's OK there's only one disease that we want to predict. And let the model learn with all the neurons learned together by creating interaction between them. Have you guys heard of Soft Max? Yes, some of you, I see that and OK, so let's, let's look at Soft Max a little bit to get so we set a new goal now. Which is we had. A constraint Which is unique. Animal on an image. So at most one animal on an image. So I'm going to modify the network a little bit. We we have our cats and there's no line on the image. We flatten it. And now I'm going to use the same scheme with the tree neurons. 8182. 83 But as an output. What I'm going to use is an exponent a softmax function. So let me be more precise. Let me, let me actually introduce another notation to make it easier. As you know the neuron is a linear part plus an activation. So we're going to introduce A notation for the linear part. I'm going to introduce Z11 to represent the linear part of the first neuron. Z1. 12 To introduce the linear part of the second neuron. So now a neuron has two parts one which computes Z and one which computes A equals smoid. Z. Now, I'm going to remove all the activations. And make this this. And I'm going to use the specific formula. So this, if you recall, it's exactly the softmax formula. Yeah. OK, so now The network we have, can you guys see where it's too small? Too small. OK, I'm going to just write this formula bigger and then you can figure out the others I guess because. E of Z 31 divided by sum from K equals 1 to 3. Of Exponential of ZK1. Can you see this one? So here is for the 3rd 1 if you are doing it for the 1st 1, you will add you just change this into a 2 into a 1 and for a second one into a 2. So why is this formula interesting and why is it not robust to this labeling scheme anymore? Is because the sum of the outputs of this network have to sum up to one. You can try it. If you sum the three outputs, you get the same thing in the numerator and on the numerator, and you get 1. That make sense? So instead of getting a probabilistic output for each Each of if each of Y 1 Y 2 Y had 3, we will get a probability distribution over all the classes so it means we cannot get 0.7, 0.6, 0.1, telling us roughly that there is probably a cat and a lion but no iguana. We have to sum this to one, so it means if there is no cats and no lion, it means there is very likely an iguana. The three probabilities are dependent on each other. And for this one we have to label. The following way, 110 for a cat, 010 for a lion, or 001 for an iguana. So this is called a softmax. Multi-class network. This You assume there is at least one of the 3 classes, otherwise you have to add the 4th input that will represent absence of animal. But this way, yeah, you assume there's always one of these three animals on every picture. And how many parameters does the network have? The same as the second one. We still have 3 neurons and although I didn't write it, this Z1 is equal to W11 X plus B1, Z2 same Z3 same. So there's 3 and plus 3 parameters. So one question that we didn't talk about is how do we train these parameters? These these parameters, the 3 and plus 3 parameters, how do we train them? You think this scheme will work or not? What's wrong? What's wrong with this scheme? What's wrong with the last function specifically? There's only 2 outcomes. So in this last function. Y is a number between 0 and 1. Yha same is a probability. Y is either 0 or 1. Y hat is between 0 and 1, so it cannot match this labeling. So we need to modify the loss function. So let's call it Lori What I'm going to do is I'm going to just sum it up for the 3 neurons. Does this make sense? So I'm just doing 3 times this loss for each of the neurons, so we have exactly 3 times this. We sum them together. And if you train this last function you should be able to train the three neurons that you have and again talking about scarcity of one of the classes if there's not many iguana. Then The 3rd term of this sum. It's not going to help this neuron train towards detecting an iguana. It's going to push it to detect no iguana. Any question on the last function? Does this one make sense? Yeah. Yeah, usually that's what will happen is that the output of this network once it's trained is going to be a probability distribution you will pick the maximum of those and you will set it to 1 and the others to 0 as your prediction. One more question, yeah. If you use the 21, if you use this labeling scheme like 110 for this network. What do you think will happen? It will probably not work and the reason is this sum is equal to 2, the sum of these entries, while the sum of these entries is equal to 1, so you will never be able to match the output to the inputs to the label. It makes sense? So what the network is probably going to do is it's probably going to send this 1 to 1/2, this 1 to 12, and this 1 to 0, probably which is not what you want. OK, let's talk about the loss function for this soft max regression. Because you know what's interesting about this loss is if I take this derivative. Derivatives of the last 3n with respect to W21. Do you think it's going to be harder than this derivative than this one or no? It's going to be exactly the same because only one of these three terms depends on W12. It means the derivative of the two others are zero, so we're exactly at the same complexity during the derivation. But this one You think if you try to compute. Let's say we we define a loss function that corresponds roughly to that if you try to compute the derivative of the loss with respect to W2. It would become much more complex. Because this number, the output here that is going to impact the loss function directly not only depends on the parameters of W2, it also depends on the parameters of W1 and W3 and same for these outputs. This output also depends on the parameters W2. Does it make sense because of this denominator. So the soft max regression needs a different loss function and a different. Derivative. So the last function we'll define is a very common one in in the learning. It's called the soft max first entropy. Cross entropy. Loss. I'm not going to to into the details of where it comes from but you can get the intuition. YK Love So it's it's surprisingly looks like the binary croissant, the binary uh the logistic loss function. The only difference is that we will sum it up. On all the. On all the classes. Now we will take a derivative of something that looks like that later uh but I'd say if you can try it at home on this one it would be a good exercise as well. So this binary cross-entropylus is very likely to be used in classification problems that are multi-class. OK, so this was the first part on logistic regression types of networks and I think we're ready now with the notation that we introduced to jump on to neural networks. Any questions on this first part before we move on? So one question I would have for you, let's say instead of trying to predict if there is a cat or no cat we were trying to predict the age of a cat. Based on the image. What would you change? This network instead of predicting 10, you want to predict the age of the cat. What are the things you would change? Yes. OK, so I repeat, I, I basically make several output nodes where each of them corresponds to one edge of cats. So would you use this network or the third one? We use the the 3 neuron neural network or the soft micro regression. The 3rd 1, why? You have a unique age. You cannot have two ages, right? So we would use a soft max one because we want a probability distribution along the edge, the edge. OK, that makes sense, um. That's a good approach. There's also another approach which is using directly regression. To predict an age, an age can be between 0 and plus plus infinity, but 0 and a certain number. And uh so let's say you wanna do a regression, how would you modify your network? Change the sigmoid. The sigmoid puts the Z between 0 and 1. We don't want this to happen. So I'd say we will change the sigmoid into what function would you change the sigmoid. Yeah, so the second one you said was? Or to get a some type of distribution. OK, so let's, let's go with linear. You, you mentioned linear. We could just use a, a linear function, right? for the sigmoid, but this becomes a linear regression. The whole network becomes a linear regression. Another one that is very common in in deep learning is called the Relou function. It's a function that is almost linear, but for every input that is negative, it's equal to 0 because we cannot have negative age. It makes sense to use this one. OK, so this is called rectified. Linear units Relu. It's a very common one in the. Now what else would you change? We talked about linear regression. Do you remember the last function you were using in linear regression? What was it? It was probably one of these two Y hat minus Y? Just comparison between the outputs label and Y hats, the prediction or it was the L2 lossy hat minus Y in L2 norm. So that's what we would use. We would modify your loss function to fit the regression type of problem. And the reason we would use this loss instead of the one we have for regression task is because in optimization the shape of this loss is much easier to optimize for a regression task than it is for a classification task and vice versa. Not going to go into the details of that but that's the intuition. OK, let's go have fun with neural networks. So we we stick to our first goal. Uh given an image. Tell us if there is a cat or no cat. This is 1, this is 0, but now we're going to make a network a little more complex. We're going to add some parameters. So I get my picture of the cats. That is moving. OK, and what I'm going to do is that I'm going to put more neurons than before. Maybe something like that. So using the same notation, you see that my square bracket here is 2 indicating that there is a layer here which is the second layer. While this one is the 1st layer and this one is the 3rd layer. Everybody's uh up to speed with the notations. Cool. So now notice that when you make a choice of architecture you have to be careful of one thing. Is that the output layer has to have the same number of neurons as it wants the number of classes to be for a classification. And one for a regression. So, um, how many parameters does this this network have? Can someone quickly give me the thought process? So how much here? Yeah, like 3 and plus 3, let's say. Yeah, correct. So in here you would have 3 weights plus 3 biases. Here you would have 2 times 3 weights plus 2 biases because you have 3 neurons connected to 2 neurons, and here you will have 2 times 1 plus 1 bias. Makes sense. So this is the total number of parameters. So you see that we didn't add too much parameters. Most of the parameters are still in the input layer. Um, let's define some vocabulary. The first word is layer. Layer denotes neurons that are not connected to each other. These two neurons are not connected to each other. These 3 neurons are not connected to each other. We call this cluster of neurons a layer, and this has 3 layers. We would use input layer to define the first layer, output layer to define the third layer because it directly sees the output, and we would call the second layer a hidden layer. And the reason we call it hidden is because the input and the outputs are hidden from this layer. It means the only thing that this layer sees as input is what the previous layer gave it. So it's an abstraction of the inputs, but it's not the inputs. Does it make sense? And saying it doesn't see the output it just gives what it understood to the last neuron that will compare the output to the ground truth. So now why are neural networks interesting and why do we call this hidden layer? Um, Is because if you train this network on cat classification with a lot of images of cats you would notice that the first layers are going to understand the fundamental concepts of the image which is the edges. This neuron is going to be able to detect these types of edges. This neuron probably going to detect some other type of edge. This neuron may be this type of edge. Then what's gonna happen is that these neurons are gonna communicate what they found on the image to the next layer neuron. And this neuron is going to use the edges that these guys found to figure out that oh there is a there are ears. Well this one is going to figure out oh there's a mouth. And so on if you have several neurons and they're going to communicate what they understood to the output neuron that is going to construct the face of the cat based on what it received and be able to tell if there is a cat or not. So the reason it's called hidden layer is because we we don't really know what it's going to figure out, but with enough data it should understand very complex information about the data. The deeper you go, the more complex information the neurons are able to understand. Let me give you another example which is a house prediction example. House price prediction. So let's assume that our inputs are number. Of bedrooms? Size of the house? ZIP code? And wealth Of the neighborhood, let's say. What we will build is a network that has 3 neurons. In the first layer and one neuron in the output layer. So what's interesting is that as a human if you were to build uh this network and like hand engineer it you would say that. Uh, OK, zip codes and wealth or, or sorry. Never mind, let's do that. Zip code and wealth are able to tell us. About the school quality in the neighborhood. The quality of the school? That is next to the house, probably. As a human you would say these are probably good features to predict that the zip code is going to tell us if the neighborhood is workable or not. Prob. The size and the number of bedrooms. Is going to tell us what's the size of the family that can fit in this house, and these three are probably better information than these in order to finally predict the price. So that's a way to hand engineer that by hand as a human in order to. Give human knowledge to the network to figure out the price. In practice, what we do here. Is that we use a fully connected. Layer. Fully connected, what does it mean? It means that we connect every input of a layer. Every input to the first layer, every output of the first layer to the input of the third layer, and so on. So all the neurons among from one layer to another are connected with each other. What we're saying is that we will let the network figure this out. We will net the neurons of the first layer, figure out what's interesting for the second layer to make the price prediction, so we will not tell this to the network. Instead we will fully connect the network. And so on. OK. We fully connect the network and let it figure out what are the interesting features and oftentimes the network is going to be able better than humans to find these what are the features that are represented. Sometimes you may hear uh neural networks referred as uh black box models. The reason is we will not understand what this edge would correspond to it's it's hard to figure out that this neuron is detecting. A weighted average of the input features. Does it make sense? Another word you might hear is end to end learning. The reason we talk about end to end learning is because we have an input. A ground truth and we don't. Constrain the network in the middle we let it learn whatever it has to learn and we call it end to end learning because we're just training based on the input and the output. In Let's delve more into the math of this network. The neural network that we have here, which has an input layer, a hidden layer, and an output layer. Let's try to write down the equations that run the input. And 4 propagated through the output. We first have Z1, that is the linear part of the first layer that is computed using W1 times X plus B. One Then this Z1 is given to an activation let's say it's sigmoid. Which is sigmoid of Z1. Z2 is then the linear part of the second neuron. Which is going to take the output of the previous layer multiplied by its weight. And other bias? The second activation is going to take the sigmoid of Z2. And finally, we have the 3rd layer which is going to multiply its weights. With the output of the layer preceding it. And add its bias? And finally, we have the 3rd activation which is simply the sigmoid of the tree. So what is interesting to notice between these equations. And the equations that we wrote here. Is that we put everything in matrices. So it means This A3 that I have here sorry this here for 3 neurons I wrote 3 equations here for. 3 neurons in the second layer. I just wrote a single equation to summarize it. But the shapes of these things are going to be vectors. So let's go over the shapes. Let's try to define them. Z11 is going to be X, which is N by 1. Time W, which has to be 3x N because it connects 3 neurons to the input. So this Z has to be 3 by 1. It makes sense because we have 3 neurons. Now let's go, let's go deeper. A1 is just the sigmoid of Z1 so it doesn't change the shape. It keeps the 3 by 1. Z2 we know it, it has to be 2 by 1 because there are two neurons in the second layer. And it helps us figure out what W2 would be. We know A1 is 3 by 1. It means that W2 has to be 2 by 3. And if you count the edges between the 1st and the 2nd layer here, you will find 6 edges. 2 times 3. A2. Same shape as Z2. Z 31 by 1831 by 1. W 3, it has to be 1 by 2 because A2 is 2 by 1. And same for me. B is going to be the number of neurons, so 3 by 1. 2 by 1. And finally one by one. So I think it's usually very helpful even when coding this type of equations to know all the shapes that are involved. Are you guys like totally OK with the shapes super easy to figure out? OK, cool. So now, What is interesting is that. We will try to vectorize the code even more. Does someone remember the difference between stochastic gradient descent and gradient descent? What's the difference? Exactly, so Casigrade in descent is. Updates the weights and the bias after you see every example so the the direction of the gradient is quite noisy doesn't represent very well the entire batch while gradient descent or batch gradient descent is update after you've seen the whole batch of examples. And the gradient is much more precise it points to the direction you wanna go to. So, What we're trying to do now is to write down these equations if instead of giving one single cat image we had given a bunch of images that either have a cat or not a cat. So now Or input X so. What happens? For an input batch. Of examples. So now our our X is not anymore a single column vector, it's a matrix. With the first image corresponding to X1, the second image corresponding to X2, and so on until the M image corresponding to XM, and I'm introducing a new notation. Which is the parenthesis superscript. Corresponding to the idea of the example. So square brackets for the layer. Round rockets for the idea of the example we're talking about. So just to give more context on what we're trying to do, we know that this is a bunch of operations we just have a, a network with inputs hidden and output layer we could have a network with 1000 layer. The more layers we have, the more computation and it quickly goes up. So what we wanna do is to be able to parallelize our code or or computation as much as possible by giving batches of inputs and parallelizing these equations. So let's see how these equations are modified when we give it a batch of inputs. I will use capital letters. To denote uh the equivalent of the lower case letters but for a batch of input. So Z1 as an example would be W1. Let's use the same actually. W1 times X plus B1. So let's analyze what Z1 would look like. The one we know that for every. For every input example of the batch, we will get 1 Z1. So it should look like this. Then we have to figure out what has to be the shapes of this equation in order to end up with this we know that Z1 was 3 by 1, it means. It means capital Z1 has to be 3 by M. Because each of these column vectors are 3 by 1 and we have M of them because for each input we forward propagate through the network we get these equations. So for the first cat image we get these equations for the second CT image we get again equations like that and so on. So what is the shape of X? We have it above. We know that it's N by N. What is the shape of W1? It didn't change. W1 doesn't change. It's not because I will give 1000 inputs to my network that the parameters are going to. Be more. So the parameter number stays the same even if I give more inputs. And so this has to be 3 by N in order to match 0. Now the interesting thing. Is that there is um an algebraic problem here. What is the algebraic problem? We said that the number of parameters doesn't change. It means that W has the same shape as it has before, as it had before. B should have the same shape as it had before, right? Should be 3 by 1. What's the problem of this equation? Exactly. We're summing a tree byM matrix to a tree by 1 vector. This is not possible in math. It doesn't work. It doesn't match when you do some summations or subtraction, you need the two terms to be the same shape because you will do an element wise addition of an element wise subscription. So what's the trick that is used here? It's a, it's a technique called broadcasting. Broadcasting is that is the fact that we don't want to change the number of parameters it should stay the same. But we still want this operation to to be able to be written in parallel version so we still want to write this equation because we want to parallelize our codes, but we don't want to add more parameters. It doesn't make sense. So what we're going to do is that we're going to create a vector B til the one which is going to be B1. Repeat it 3 times. Uh sorry, repeated M times. So we just keep the same number of parameters but just repeat them in order to be able to write my code in parallel. Is this called broadcasting. And what is convenient is that for those of you who do not do homeworks are in Malab or Python. MATLAB. OK, so in MATLAB, no Python, um, Python. So in Python there is a package that is often used to to code these equations. It's NumPy. Some people call it numpi, not sure. So NumPy. Basically numerical Python we directly do the broadcasting it means if you sum this. 3 by M matrix with a 3 by 1 parameter vector is going to automatically reproduce the parameter vector M times so that the equation works. It's called broadcasting. Did it make sense? So because we're using this technique we're able to rewrite all these equations with capital letters. Do you wanna do it together or do you wanna do it on your own? Who wants to do it on their own. OK, so let's do it on their own. On your own, so rewrite these with capital letters and figure out the shapes. I think you can do it at home. We're we're not going to do here, but make sure you understand all the shapes, yeah. So the question is how is this different from principal component analysis? This is a supervised learning algorithm that will be used to predict the price of a house. Principal component analysis doesn't predict anything. It gets, uh, inputs uh matrix X, normalizes it, uh, computes the covariance matrix, and then figures out what are the principal components by doing the the eigenvalue decomposition. But the outcome of PCA is, you know that the most important features of your data set X. Are going to be these features here we're not looking at the features we're only looking at the outputs that's what is important to us. Yes. So the question is, uh, can you explain why the first layer would see the edges? Is there any intuition behind it? It's not always going to see the edges, but it's often time going to see the edges because, uh, in order to detect a human face, let's say you will train an algorithm to find out whose face it is so it has to understand the faces very well. Um, you need the network to be complex enough to understand very detailed feature of the face and usually. This neuron, what it sees as inputs are pixels. So it means every edge here is the multiplication of the weight by a pixel. So it sees pixels. It cannot understand the face as a whole because it sees only pixels it's very granular information for it so it's going to. Check if pixels nearby have the same color, and understand that there's an edge there. OK, but it's too complicated to understand the whole face in the first layer. However, if it understands a little more than a pixel information, it can give it to the next neuron. This neuron will receive more than pixel information it will receive a little more complex like edges. And then it will use this information to build on top of it. And build the features of the face. So what I'm trying to sum up is that these neurons only see the pixels, so they're not able to build more than the edges. That's the minimum thing that they can, the maximum thing they can build. And it's, it's a complex topic like interpretation of neural network is a highly researched topic, a big research topic so nobody figured out exactly how all the neurons evolve. One more question and then we move on. I How many years So the question is how How do you decide how many neurons per layer, how many layers, what's the architecture of your neural network? There are two things to take into consideration I would say. First, nobody knows the right answer, so you have to test it. So you, you guys talked about training sets, validation set and test set. So what we would do is we would try 10 different architectures, train it train the network on this, look at the validation sets accuracy of all these, and decide which one seems to be the best. That's how we figure out what's the right network size. On top of that, using experience is often valuable. So if you give me a problem, I try always to gauge how complex is the problem like cat classification. Do you think it's easier or harder than day and night classification? So day and night classification is I give you an image. I ask you to predict if it was taken during the day or during the night, and on the other hand, you want to detect if there's a cat on the image or not, which one is easier, which one is harder. Who thinks cat classification is harder? OK, I think people agree cat classification seems harder. Why? Because there are many breeds of cats can look like different things. There's not many breeds of knights, uh, I mean, I guess. Uh, one thing that might be challenging in day night classification is if you want also to figure it out in house like inside, you know, maybe there is a tiny window there and I'm able to tell that it is the day, but for a network to understand it you will need a lot more data than if only you wanted to work outside different so these problems all have their own complexity based on their complexity. I think the network should be deeper. The com the more complex usually is the problem, the more data you need in order to figure out the output, the more deeper should be the network. That's an intuition, let's say. OK, let's move on guys because I think we have. About what 12 more minutes? OK, let's try to write the loss function. For this problem. So now that we have our network we have written this propagation equation and I will call it forward propagation because it's going forward it's going from the input to the output. Later on when we will we will derive these equations we will call them backward propagation because we're starting from the loss and going backwards. So let's let's talk about the optimization problem. Optimizing. W1 W2 W 3 B1 Me too Me, we have a lot of stuff to optimize, right? We have to find the right values for this and remember model equals architecture plus parameter we have our architecture. If we have our parameters we're done. So in order to do that we have to define. An objective function. Sometimes called loss, sometimes cause cost function. So usually we would call it loss if there is only one example in the batch and cost if there is multiple examples. In a match. So the last function. That let let's define the cost function. The cost function J depends on why hat and why. OK, so why hats? Why hat is a tree? OK. It depends on why hats and why. And we will set it to be. The some Of the loss functions LI and I will normalize it, it's not mandatory but normalize it with one over N. So what does this mean is that we're going for batch gradient descent. We want to compute the loss function for the whole batch, paralyze our code. And then calculate the cost function that will be then derived to give us the direction of the gradients that is the average direction of all the the the the derivation with respect to the whole input batch. And LI Will be the last function corresponding to one parameter. So what's the error on this specific uh one input, sorry, not parameter. And it will be the logistic loss. You've already seen these equations I believe things like that. So now is it more complex to take a derivative? With respect to J, like of J with respect to the parameters or of L? What's the most complex between This one Let's say we're taking directly with respect to W2 compared to this one. Which one is the hardest? Who thinks Jay is the hardest? We think it doesn't matter. It doesn't matter. Because derivation is is a linear operation, right? So you can just take the derivative inside and you will see that if you know this. You just have to take the sum over this. So instead of computing all derivatives on J, we will compute them on L but it's totally equivalent. There's just one more step at the end. OK, so now we defined our last function. Super. We define our last function and the next step is optimize so we have to compute a lot of derivatives. And that's called backward propagation. So the question is why is it called backward propagation? Is because what we want to do ultimately is this. For any L equals 1 to 3. We want to Do that, w l equals wl. Minus alpha derivative of J. With respect to WL. And BL equals BL minus alpha derivative of J with respect to BL. So we want to do that for every parameter in layer 12, and 3. So it means we have to compute all these derivatives. We have to compute derivatives of the cost with respect to W1, W2, W3, B1, B2, B3. You've done it with logistic regression we're going to do it with a neural network and you're going to understand why it's called backward propagation. Which one do you want to start with? Which derivative you want to start with the derivative with respect W1, W2, or W3, let's say. Assuming we'll do the bias later. Do what? W1? Do you think W1 is a good idea? I, I don't want to do that one. So I think we should do W3 and the reason is because if you look at this loss function. Do you think the relation between W3 and this last function is easier to understand or the relation between W1 and this loss function? Is the relation between W3 and this last function because W3 happens much later in the in the network. So if you want to understand how much should we move W1 in order to make the last move, it's much more complicated than answering the question how much should W3 move to move the loss. Because there's much more connections. If you want to compete with W1, so that's why we call it backward propagation is because we will start with the top layer, the one that's the closest to the loss function, derive the derivative of. Jay, With respect to W1. OK, and once we computed this derivative which we're going to do next week. Once we computed this number, we can then tackle this one. Oh, sorry, yeah, thanks. Thank you, yeah, once we computed this number we will be able to compute this one. Very easily. Why very easily? Because we can use the chain rule of calculus, so let's see how it works. We're we're, I'm just going to give you uh the one minute pitch on on backdrop but uh we'll do it next week together so if we had to compute this derivative. What I will do is that I will separate it into several derivatives that are easier. I will separate it into derivatives of J with respect to something with this something with respect the W3. And the question is What should this something be? I will look at my equations. I know that Jay depends on white hats. And I know that Y depends on Z3. Y is the same thing as A3. I know it depends on Z3. So why don't, why don't I include Z3 in my equation? I also know that Z3 depends on W3 and the derivative of Z3 with respect to W3 is super easy. It's just A2 transpose. So I will just make a quick hack and say that this derivative is the same as taking it with respect to A3. Taking the derivative of A3 with respect to Z3. And taking the derivatives of these 3. With respect to W3. So you see same same derivative calculated in different ways. And I know these. I know these are pretty easy. To compute. So that's why we call it back propagation is because I will use the chain rule to compute the derivative with W3 and then when I wanna do it for W2 I'm going to insert. I'm going to insert the derivative with Z3. Time is the derivative of Z3. With respect to A2 times the derivative of A2, with respect to Z2. Time derivative of Z2. With respect to W2. Does this make sense that This thing here. Is the same thing as this? It means if I want to compute the derivative of W2, I don't need to compute this anymore. I already did for W3. I just need to compute those which are easy ones. And so on. If I want to compute the derivative of J with respect to W1. I'm going to, I'm not going to decompose all the thing again. I'm just going to take the derivative of J with respect to Z2. Which is equal to this whole thing? And then I'm gonna multiply it by a derivative of Z2. With respect to A1 times the derivative of A1, with respect to Z1 times the derivative of Z1 with respect to W1. And again this thing I know it already. I computed it previously just for this one. So what's what's interesting about it is that I'm not gonna redo the work I did. I'm just gonna store the right values while back propagating and continue to derivate. One thing that you need to notice though is that. Look you need this forward propagation equation in order to remember what should be the path to take in your chain rule. Cause you know that's This derivative of J with respect to W3, I cannot use it as it is because W3 is not connected to the previous layer. If you look at this equation. A2 doesn't depend on W3. It depends on Z3 sorry, like, uh, my bad. It depends, no, sorry, what I wanna say is that. Z2 It's connected to W2. But A1 is not connected to W2. So you wanna choose the path that you're going through in the proper way so that there's no cancellation in these derivatives. You cannot compute derivative of W. 2, with respect to to a 1. Right, you cannot compute that. You don't know it. OK, so I think we're done for today so one thing that I'd like you to do if you have time is just think about the things that can be tweaked in a neural network. When you build a neural network you're not done. You have to tweak it you have to tweak the activations you have to tweak the loss function there's many things you can tweak and that's what we're going to see next week. OK, thanks.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bedrock_client = boto3.client(\"bedrock-runtime\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"mcqs\": [\n",
      "    {\n",
      "      \"question\": \"What type of algorithm is logistic regression primarily used for?\",\n",
      "      \"options\": [\"Classification\", \"Regression\", \"Clustering\", \"Dimensionality Reduction\"],\n",
      "      \"answer\": \"Classification\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"In a neural network, what do we call neurons that are not connected to each other in the same group?\",\n",
      "      \"options\": [\"Cluster\", \"Layer\", \"Hidden Units\", \"Activation Group\"],\n",
      "      \"answer\": \"Layer\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What is the purpose of the softmax function in neural networks?\",\n",
      "      \"options\": [\"To normalize outputs between 0 and 1\", \"To create a probability distribution that sums to 1\", \"To activate neurons\", \"To compute gradients\"],\n",
      "      \"answer\": \"To create a probability distribution that sums to 1\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What is broadcasting in the context of neural networks?\",\n",
      "      \"options\": [\"Sending data between layers\", \"Repeating parameter vectors to match dimensions\", \"Broadcasting radio signals\", \"Sharing weights between neurons\"],\n",
      "      \"answer\": \"Repeating parameter vectors to match dimensions\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"Why is the process called backward propagation?\",\n",
      "      \"options\": [\"Because we move backwards in time\", \"Because we compute derivatives from the output layer towards the input layer\", \"Because we reverse the neural connections\", \"Because we process data in reverse\"],\n",
      "      \"answer\": \"Because we compute derivatives from the output layer towards the input layer\"\n",
      "    }\n",
      "  ],\n",
      "  \"text_questions\": [\n",
      "    {\n",
      "      \"question\": \"Explain how the first layer of a neural network processes image data and what types of features it typically learns to detect.\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"What is the difference between stochastic gradient descent and batch gradient descent?\"\n",
      "    },\n",
      "    {\n",
      "      \"question\": \"How do you determine the appropriate architecture (number of layers and neurons) for a neural network for a given problem?\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "def generate_quiz(transcript, model_id=\"anthropic.claude-3-5-sonnet-20241022-v2:0\"):\n",
    "    \"\"\"\n",
    "    Uses AWS Bedrock with Claude models to generate quiz questions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    transcript : str\n",
    "        The text transcript from which to generate the quiz.\n",
    "    model_id : str, optional\n",
    "        The Claude model ID to use. Default is Claude 3.5 Sonnet v2.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        The generated quiz in JSON format.\n",
    "    \"\"\"\n",
    "    # Initialize the Bedrock runtime client\n",
    "    bedrock_client = boto3.client(\"bedrock-runtime\", region_name=\"us-west-2\")\n",
    "\n",
    "    # Claude models use the Messages API format\n",
    "    body = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 2000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"Generate a quiz from this transcript:\\n{transcript}\\n\"\n",
    "                          \"The quiz should include:\\n\"\n",
    "                          \"- 5 Multiple Choice Questions (MCQs)\\n\"\n",
    "                          \"- 3 Short Answer Questions\\n\"\n",
    "                          \"- Format the output in JSON.\\n\"\n",
    "                          \"Example Output:\\n\"\n",
    "                          \"{\\n\"\n",
    "                          \"  \\\"mcqs\\\": [\\n\"\n",
    "                          \"    {\\\"question\\\": \\\"What is AI?\\\", \\\"options\\\": [\\\"Artificial Intelligence\\\", \\\"Automated Input\\\", \\\"None\\\"], \\\"answer\\\": \\\"Artificial Intelligence\\\"}\\n\"\n",
    "                          \"  ],\\n\"\n",
    "                          \"  \\\"text_questions\\\": [\\n\"\n",
    "                          \"    {\\\"question\\\": \\\"Explain how AI models learn?\\\"}\\n\"\n",
    "                          \"  ]\\n\"\n",
    "                          \"}\"\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    try:\n",
    "        response = bedrock_client.invoke_model(\n",
    "            modelId=model_id,\n",
    "            contentType=\"application/json\",\n",
    "            accept=\"application/json\",\n",
    "            body=body\n",
    "        )\n",
    "\n",
    "        # Read and parse the response\n",
    "        response_body = json.loads(response['body'].read().decode('utf-8'))\n",
    "        \n",
    "        # Extract the response text from the Claude message structure\n",
    "        return response_body.get('content', [{}])[0].get('text', '')\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error invoking model: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example Usage\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    quiz_result = generate_quiz(transcript)\n",
    "    print(quiz_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
